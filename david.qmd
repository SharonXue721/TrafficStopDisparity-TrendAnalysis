---
title: "David's Notes"
format: html
editor: visual
execute:
  message: false
  eval: false
---

-   Driver info

-   location

-   officer info

## Data Acquisition

```{r}
library(tidyverse)
library(rvest)

# read in the data site
data_site <- read_html("https://openpolicing.stanford.edu/data/")

# extract the names of the states and the URLs for the CSV data files
state_names <- 
  data_site %>% 
  html_elements(".state-title") %>% 
  html_text2() %>% 
  str_remove_all("\\t")

csv_data_urls <- 
  data_site %>% 
  html_elements("[data-title='Download']") %>% 
  html_elements("a") %>% 
  html_attr("href")  %>% 
  str_subset(".csv")

# create a dataframe with the state names and their corresponding CSV data URLs
full_data <- 
  data_site %>% 
  html_table() %>% 
  bind_rows() %>% 
  janitor::clean_names() %>% 
  select(city = state) %>% 
  mutate(state = if_else(city %in% state_names, city, NA)) %>% 
  fill(state, .direction = "down") %>% 
  filter(!city %in% state_names) %>% 
  add_column(data_link = csv_data_urls)

```

Cool. Now we programmatically download the data

```{r}

# increase the timeout of your R session if necessary
options(timeout = 180)

# set the location where you want the files to be downloaded to
location = Sys.getenv("LOCATION")

# check which ones already exist
files_to_download = setdiff(basename(csv_data_urls), list.files(location,  pattern = "*\\.zip"))

actual_links = csv_data_urls[sapply(csv_data_urls, \(x) any(str_detect(x, files_to_download) ))]

# download the zip files for the ones that don't already exist

invisible(sapply(actual_links, \(x) download.file(x, paste0(location, basename(x)), mode = "wb",quiet = T)))

# unzip the files

invisible(sapply(list.files(location, full.names = T, pattern = "*\\.zip"), \(x) unzip(zipfile = x, overwrite = T, exdir = location)))

# remove all zip files

unlink(list.files(location, full.names = T, pattern = "*\\.zip$"), recursive = TRUE)
```

The total files make up 47 gigabytes, which is MASSIVE. Let's shrink things using the `parquet` format.
Since the data doesn't have uniquely identified columns within the dataset, I also want to keep track of the state and city, which is included in the file name

```{r}
library(arrow)
library(duckdb)


# save the csv files in parquet format

files_to_convert = list.files(location, full.names = T, pattern = "*\\.csv$")

file_meta_data <- 
  list.files(location, pattern = "*\\.csv$") %>% 
  tibble() %>% 
    separate_wider_regex(
      cols = everything(),
      patterns = c(
        #base_path = "^.+",
        state = ".{2}", "_", city = ".+", "_",
        last_updated = "\\d{4}_\\d{2}_\\d{2}", "\\.csv$"), 
      cols_remove = F) %>% 
  rename(file_name = ".")

SOP <- arrow::open_csv_dataset(sources = files_to_convert)

SOP %>% glimpse()
```

I'm pretty stuck now since the data doesn't have all of the columns that I need to differentiate
them, and i super don't want to open and close them all to re-add them later. So I'm gonna do something else now.

